{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 1us/step\n",
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all \n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/2\n",
      "7620/7619 [==============================] - 90s 12ms/step - loss: 5.6238 - accuracy: 0.1460 - val_loss: 5.2678 - val_accuracy: 0.1575\n",
      "Epoch 2/2\n",
      "7620/7619 [==============================] - 89s 12ms/step - loss: 5.4227 - accuracy: 0.1667 - val_loss: 5.6715 - val_accuracy: 0.1683\n",
      "<START> please give this one it's a great thing there has an particularly long telling me the same the film \n",
      "<START> this film requires a viewing for teacher dentist br br revenge that's his girl the remember awful show the \n",
      "<START> many animation buffs consider the late gore so much anyway this really show voice this is <UNK> family these \n",
      "<START> i generally love this script which is far a pretty between the up and the movies cinematography why everyone \n",
      "<START> like some other people and horror <UNK> video the performance of friends <UNK> romance sign head she almost someone \n",
      "<START> i'm absolutely <UNK> this film we won't say that there are yet these people than i annoying keep the \n",
      "<START> originally supposed to be run what impression br br very different direction feel so you're <UNK> in the several \n",
      "<START> the <UNK> richard <UNK> leader will good around the original all these <UNK> shows all running in the pretty \n",
      "<START> hollywood had a long down to be make a huge cast <UNK> time so jackson and they trip to \n",
      "<START> this film is where entire five years is realize that if a time over a <UNK> <UNK> more somewhere \n",
      "<START> inspired by <UNK> <UNK> work as many early such as re indeed <UNK> i gives the leave watching i \n",
      "<START> when i first saw this movie along i was suspense and making him for such a movie to be \n",
      "<START> oh dear oh dear in <UNK> photography hilarious wearing both her known as few <UNK> the script is said \n",
      "<START> i started watching this frame and people who also michelle release would have been funny at the way but \n",
      "<START> a touching documentary that that please done <START> head up so often ground leave the <UNK> <UNK> flesh <UNK> \n",
      "<START> let me first start they <UNK> an <UNK> i was well i've like <START> exactly <UNK> the <UNK> the \n",
      "<START> from 1996 first i had a <UNK> coming with film she was pretty bad by them we stupid at \n",
      "<START> ed <UNK> mitchell is <UNK> no sort of here to <UNK> sheer sort of take <UNK> towards a <UNK> \n",
      "<START> i was really <UNK> china he don't go out to make a needed to be <UNK> <UNK> that guy \n",
      "<START> right so you have been <UNK> on set with mediocre films at the character can't be closing to far \n",
      "==== 3\n",
      "3 1.0\n",
      "4 0.70285857\n",
      "8 0.6927783\n",
      "7 0.6866499\n",
      "5 0.6483408\n",
      "ten 0.6345316\n",
      "25 0.62346023\n",
      "five 0.6232257\n",
      "15 0.6204138\n",
      "30 0.6143494\n",
      "==== two\n",
      "two 0.9999999\n",
      "three 0.72133744\n",
      "four 0.70503163\n",
      "many 0.70177704\n",
      "few 0.6664232\n",
      "several 0.64303225\n",
      "these 0.6193046\n",
      "some 0.61904025\n",
      "those 0.5954154\n",
      "modern 0.5685613\n",
      "==== great\n",
      "great 1.0\n",
      "good 0.6721209\n",
      "decent 0.593312\n",
      "fine 0.588979\n",
      "bad 0.5847655\n",
      "excellent 0.56626374\n",
      "terrible 0.5576192\n",
      "wonderful 0.54989105\n",
      "brilliant 0.53579056\n",
      "supporting 0.5200755\n",
      "==== money\n",
      "money 1.0\n",
      "laugh 0.55732155\n",
      "apart 0.5311364\n",
      "effort 0.5283829\n",
      "reviews 0.5066591\n",
      "connection 0.502812\n",
      "books 0.49698466\n",
      "stuff 0.49290538\n",
      "comment 0.49088818\n",
      "happened 0.4902538\n",
      "==== years\n",
      "years 1.0\n",
      "year 0.6712697\n",
      "days 0.5545432\n",
      "chemistry 0.54245496\n",
      "special 0.5215893\n",
      "once 0.51851785\n",
      "10 0.511969\n",
      "sit 0.48876807\n",
      "right 0.4749228\n",
      "sat 0.46613976\n",
      "==== look\n",
      "look 1.0\n",
      "looked 0.71553046\n",
      "looks 0.6957617\n",
      "sounds 0.6461819\n",
      "feels 0.5962646\n",
      "looking 0.5841572\n",
      "laugh 0.55721587\n",
      "talk 0.55321026\n",
      "works 0.551787\n",
      "laughing 0.5398443\n",
      "==== own\n",
      "own 1.0\n",
      "real 0.5432925\n",
      "his 0.50151306\n",
      "poor 0.5014256\n",
      "their 0.49948362\n",
      "your 0.48811015\n",
      "our 0.47902095\n",
      "personal 0.45427787\n",
      "her 0.45390242\n",
      "my 0.4504265\n",
      "==== us\n",
      "us 1.0\n",
      "words 0.42938098\n",
      "him 0.39877838\n",
      "plan 0.3928662\n",
      "move 0.384143\n",
      "needed 0.37987915\n",
      "things 0.37853423\n",
      "learning 0.37507755\n",
      "anything 0.37427393\n",
      "seemed 0.37418854\n",
      "==== using\n",
      "using 1.0\n",
      "within 0.70821923\n",
      "holding 0.70390314\n",
      "kills 0.6991261\n",
      "against 0.69820255\n",
      "finding 0.6974145\n",
      "including 0.6916106\n",
      "leaving 0.69010055\n",
      "upon 0.685984\n",
      "despite 0.6849816\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.datasets import imdb\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "from keras.utils import to_categorical, np_utils\n",
    "import numpy as np\n",
    "\n",
    "train_reviews = 5000\n",
    "valid_reviews = 100\n",
    "max_features = 5000\n",
    "embedding_size = 256\n",
    "step_size = 5\n",
    "batch_size = 32\n",
    "index_from = 2\n",
    "rnn_units = 128\n",
    "epochs = 2\n",
    "word_index_prev = {'<PAD>': 0, '<START>': 1, '<UNK>': 2}\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, index_from=index_from)\n",
    "\n",
    "word_index = {word: (index + index_from) for word, index in imdb.get_word_index().items() if (index + index_from) < max_features}\n",
    "word_index.update(word_index_prev)\n",
    "\n",
    "index_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "def print_sentence(sentence):\n",
    "    for index in sentence:\n",
    "        print(index_word[index], end=\" \")\n",
    "    print()\n",
    "\n",
    "print_sentence(x_train[0])\n",
    "\n",
    "data_train = [t for s in x_train[:train_reviews] for t in s]\n",
    "data_valid = [t for s in x_train[train_reviews:train_reviews+valid_reviews] for t in s]\n",
    "\n",
    "def batch_generator(data, batch_size, step_size):\n",
    "    seg_len = len(data) // batch_size\n",
    "    steps_per_epoch = seg_len // step_size\n",
    "    data_seg_list = np.asarray([data[int(i*seg_len):int((i+1)*seg_len)] for i in range(batch_size)])\n",
    "    data_seg_list\n",
    "    i = 0\n",
    "    while True:\n",
    "        x = data_seg_list[:, int(i*step_size):int((i+1)*step_size)]\n",
    "        y = np.asarray([to_categorical(data_seg_list[j, int(i*step_size+1):int((i+1)*step_size+1)], max_features) for j in range(batch_size)])\n",
    "        yield x, y\n",
    "        i += 1\n",
    "        if i >= steps_per_epoch:\n",
    "            i = 0\n",
    "\n",
    "w = Input(shape=(step_size,), name='Input')\n",
    "x = Embedding(input_dim=max_features, output_dim=embedding_size, name='Embedding')(w)\n",
    "y = LSTM(units=rnn_units, return_sequences=True, dropout=0.5, recurrent_dropout=0.5, name='LSTM')(x)\n",
    "w_next = TimeDistributed(Dense(units=max_features, activation='softmax', name='Dense'), name='TimeDistributed')(y)\n",
    "\n",
    "model = Model(inputs=[w], outputs=[w_next])\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "gen_train = batch_generator(data_train, batch_size, step_size)\n",
    "gen_valid = batch_generator(data_valid, batch_size, step_size)\n",
    "\n",
    "steps_per_epoch_train = len(data_train) / batch_size / step_size\n",
    "steps_per_epoch_valid = len(data_valid) / batch_size / step_size\n",
    "\n",
    "model.fit_generator(generator=gen_train, steps_per_epoch=steps_per_epoch_train, epochs=epochs,\n",
    "                    validation_data=gen_valid, validation_steps=steps_per_epoch_valid)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.log(preds) / temperature\n",
    "    preds = np.exp(preds) / np.sum(np.exp(preds))\n",
    "    choices = range(len(preds))\n",
    "    return np.random.choice(choices, p=preds)\n",
    "\n",
    "def sample_sentences(num_sentences, sample_sent_len = 20):\n",
    "    for x_test_i in x_test[:num_sentences]:\n",
    "        x = np.zeros((1, step_size))\n",
    "        sentence = x_test_i[:step_size]\n",
    "\n",
    "        for i in range(sample_sent_len):\n",
    "            for j, index in enumerate(sentence[-step_size:]):\n",
    "                x[0, j] = index\n",
    "            preds = model.predict(x)[0][-1]\n",
    "            next_index = sample(preds)\n",
    "            sentence.append(next_index)\n",
    "\n",
    "        print_sentence(sentence)\n",
    "\n",
    "sample_sentences(num_sentences=20, sample_sent_len=15)\n",
    "\n",
    "norm_weights = np_utils.normalize(model.get_weights()[0])\n",
    "\n",
    "def print_closest_words(word, nb_closest=10):\n",
    "    index = word_index[word]\n",
    "    distances = np.dot(norm_weights, norm_weights[index])\n",
    "    c_indexes = np.argsort(np.squeeze(distances))[-nb_closest:][::-1]\n",
    "    for c_index in c_indexes:\n",
    "        print(index_word[c_index], distances[c_index])\n",
    "\n",
    "words = [\"3\",\n",
    "         \"two\",\n",
    "         \"great\",\n",
    "         \"money\",\n",
    "         \"years\",\n",
    "         \"look\",\n",
    "         \"own\",\n",
    "         \"us\",\n",
    "         \"using\",\n",
    "        ]\n",
    "\n",
    "for word in words:\n",
    "    if word in word_index:\n",
    "        print('====', word)\n",
    "        print_closest_words(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu] *",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
